# 1 深度学习革命（The Deep Learning Revolution）

机器学习是当今最重要的，发展速度最快的科技领域。机器学习的应用正变得无处不在，从数据中学习解决方案在一步步取代人工设计的算法。这不仅提高了现有技术的性能，而且已经打开了新世界的大门，这对于人工设计的算法来说是不可想象的。

深度学习作为机器学习的一个特殊的分支，已经成为一个强大且通用的从数据中学习的框架。深度学习基于一种被称为 _神经网络_ 的计算模型，其灵感来自于人脑的学习和信息处理的机制。人工智能一直在寻求重现大脑的强大能力，如今人工智能和机器学习这两个名词经常互换使用。目前使用的很多 AI 系统都代表了一种用于解决某个特定问题的机器学习应用，虽然这非常有用，但远不及人类大脑。这引出了通用人工智能 Artificial General Intelligence（AGI），即建立一种更加灵活的机器。经过多年的研究，机器学习已经进入了快速发展的阶段。最近，被称为大语言模型的大型深度学习系统已经开始表现出不可思议的能力，这被认为是通用人工智能的标志。

## 1.1 深度学习的冲击（The Impact of Deep Learning）

我们通过思考四个不同领域的例子来说明机器学习的巨大适用性并解释一些术语和基本概念。值得注意的是，这些例子和其他许多应用都是使用相同的深度学习框架的变体来实现的。这和传统使用广泛不同和特定技术方法大相径庭，应该说明的是我们所选择的例子只是深度学习广泛应用的一小部分，几乎每一个依赖于计算的领域都可以得益于深度学习的变革性影响。

### 1.1.1  医学诊断（Medical diagnosis）

第一个要介绍的机器学习应用是皮肤癌的诊断问题。黑色素瘤是最危险的皮肤癌种类，但是如果及早发现是可以治愈的。`图 1.1` 展示了皮肤病灶一些实例图，上面一行是黑色素瘤而下面一行是良性痣。分别这两类图片是一个巨大的挑战，想要人工设计一种以合理的准确度来有效地分类的算法几乎是不可能的。
![Fig1.1](images/fig1.1.png)
> Figure 1.1：上面一行是和皮肤癌相关的恶性黑色素瘤，下面一行是良性痣。没有经过训练是很难区分两者的。

这个问题已经使用深度学习有效地解决了（Esteva et al.,2017）。通过使用了大量的皮肤病灶图像作为训练集，每一张图都通过活体组织检查被标记为恶行或良性，以确定其真正的类别。这个训练集用于训练一个具有 25 万参数，或者说权重的深度神经网络。通过数据来确定参数的过程被称为 _学习_ 或 _训练_。训练好的模型的目标是仅通过新的皮肤病灶的图片来预测其类别而不需要费时费力的活体组织检查。这是一个典型的 _监督学习_ 问题，因为对于每个训练样本，模型都被告知了正确的类别。这也是一个典型的 _分类_ 问题，因为每个输入都被映射到离散的分类集（在这里是恶性或良性）。如果输出是连续的值则被称为 _回归_ 问题，一个回归问题的例子就是化工生产中的产量预测，输入包括温度、压力和反应物浓度。

这个应用一个有趣的方面是可用的带标记的图片数量大概 129,000 相对来说是比较小的，因此深度神经网络首先在一个拥有 128 万张日常图片（例如狗、建筑和蘑菇）的大型数据集上进行训练，然后在皮肤病灶上进行 _微调_。这是一个 _迁移学习_ 的例子，即首先在日常图片上学习通用的特性，然后针对特定的皮肤病灶问题进行特殊训练。通过使用深度学习，皮肤病灶图像的分类已经达到了皮肤科医生的精确度（Brinker et al.,2019）。


### 1.1.2 蛋白质结构（Protein structure）

蛋白质时常被称作生命的构建基础。它们是包含一条或多条被称为氨基酸链的生物分子，有 21 种不同的类型的氨基酸，而蛋白质是由氨基酸序列确定的。一旦一个蛋白质在细胞内被合成，就会折叠成一个复杂的三维结构，正是这种结构决定了蛋白质的行为和作用。半个世纪以来，通过给定氨基酸序列，计算这种 3D 结构，一直是生物学一个基础的公开问题，指导深度学习出现才取得了相对小的进展。

3D 结构可以通过例如 X 射线、低温电子显微镜或者核磁共振波谱法等方法在实验上进行测量。然而这可能非常耗费时间，并且对某些蛋白质来说可能具有挑战性，例如可以获取纯净的样本或者由于或者其结构依赖于环境。而相反的是测量蛋白质的氨基酸序列则具有较低的成本和较高的效率。因此为了更好的理解生理过程或者研发药物，直接从氨基酸序列来预测蛋白质的 3D 结构的方法一直非常吸引人。如果训练集包含一组已知的蛋白质的氨基酸序列和对应的 3D 结构，那么一个深度学习模型可以使用氨基酸序列作为输入，使用 3D 结构作为输出来训练。蛋白质结构预测就是另一个经典的监督学习。一旦这个系统被训练，就可以接受一个新的氨基酸序列作为输入，预测相应的 3D 结构（Jumper et al.,2021）。图 1.2 比较了预测的蛋白质 3D 结构和 X 射线得到的基本真实结构。

![Figure 1.2](images/fig1.2.png)
> Figure 1.2：被称为 T1044/6VR4 的蛋白质的 3D 结构。绿色的部分表示了 X 射线得到的基本真实结构，叠加的蓝色结构表示由 AlphaFold 深度学习模型得到的预测。

### 1.1.3 图片合成（Image synthesis）

在上面讨论的两个应用中，神经网络学习如何从输入（皮肤图片或氨基酸序列）转换到输出（病灶分类或 3D 蛋白质结构）。我们现在看另一个例子，训练数据只是由一组图片样本组成，目标是让训练好的模型能够创造出相同风格的新图片。这是一个 _无监督学习_ 的例子，因为这个例子和病灶分类或蛋白质结构的例子正好想法，图片样本没有对应的标签。图 1.3 展示了一个使用在平实背景下拍摄的人类面部图片的数据集训练的神经网络生成的一些图片。这种合成图片有极高的品质并且很难喝真实的人类区分开。

![Figure 1.3](images/fig1.3.png)
> Figure 1.3：使用无监督学习训练的神经网络生成的面部图像。From https://generated.photos

这是一个 _生成式模型_ 的例子，因为它能够以相同的统计特性生成和训练数据不同的输出示例。这种方法的变体允许基于输入 _提示_ 文本来生成图像，从而使图像内容反映输入文本的含义。_生成式 AI_ 被用于描述那些能够生成图像、视频、音频、图片、候选药物分子或其他事物的深度学习模型。

### 1.1.4 大语言模型（Large language models）


机器学习近期最重要的进展之一就是发展出了一种可以处理自然语言或其他像源代码等序列数据的强大模型。_大语言模型_ 或者说 LLM（Large Language Models），使用深度学习来建立丰富的内部表示从而可以捕获语言的语义属性。一种最重要的大语言模型类别叫做 _自回归_ 语言模型，可以生成语句，因此他们是声称是人工智能的一种形式。这种模型使用单词序列作为输入，而在输出时，生成该单词序列的下个一单词。这个在结尾添加了新单词的增广序列，可以再被输入到模型中来生成后续的单词，这种处理可以被一直重复从而生成一个长的单词序列。这种模型同样也可以生成一个特殊的‘停止’词来结束文本生成，从而允许它生成有限长度的文本后停止。基于这点，用户可以在将完整的单词序列反馈给模型前，加上自己的单词从而触发进一步的文本生成。通过这种方式，人们就可以和神经网络进行对话。

这种模型在大型文本数据集上训练，数据集是随机选取的单词序列和已知的该序列的下一个单词作为输入输出对。这也是一种 _自监督学习_，即学习从输入到输出的映射，但是输出的标签是从输入数据中自动获得的，不需要人类标记。由于可以从很多来源获得大量的文本，这种方法允许扩展到非常大的训练集并且使用非常大的神经网络。

大语言模型可以建立非常强大的能力，被描述为新兴的通用人工智能的早期迹象（Bubeck et al.,2023）。我们将会在本书的最后一节讨论这种模型。在下面我们使用被称作 GPT-4（OpenAI,2023）的模型作为语言生成的例子，使用 'Write a proof of the fact that there are infinitely many primes; do it in the style of a Shakespeare play through a dialogue between two parties arguing over the proof.' 作为输入提示。
> 写出素数有无穷多个这一事实的证明；通过两方就证明进行争论的对话，以莎士比亚戏剧的风格来做到这一点。


|GPT-4: Proof of infinitely many primes|
|-|
FIRST PARTY: Thou dost not understand, my friend, There simply is no upper end To primes, those numbers indivisible, No matter if it seems impossible. 
SECOND PARTY: But how dost thou so surely know? To prove this truth, what canst thou show? 
FIRST PARTY: Consider this, my doubtful peer, A clever proof that will make clear: Assume that there’s a final prime, The largest one we’ll see in time. Let’s call this number "P" for short, And then, my friend, we will purport To build a number "Q" that’s new, One larger than "P" and prime too. 
SECOND PARTY: But how, my friend, will this be done? The proof is not yet fairly won. 
FIRST PARTY: It’s simple, friend, just wait and see, We’ll multiply the primes, that’s key. All primes from two to "P" combined, Will make this new "Q" we will find. But one thing more we must include, To make the proof more watertight, We’ll add a one onto the end, And now the proof is in our sight. 
SECOND PARTY: I see it now, it’s crystal clear, For "Q" must be a prime, my dear. It can’t divide by any prime, All smaller than itself in time. And since we chose "P" as the last, This proves that our assumption’s past. There is no final prime, we see, The numbers go on endlessly.

## 1.2 教程示例（A tutorial example）

对于机器学习领域的新人来说，许多基础的概念和大量的术语可以在一个简单的示例中介绍，这个实例涉及到一个小型合成数据集的多项式拟合（Bishop,2006）。这是一个监督学习，通过给定输入变量，可以对一个目标变量进行预测。

### 1.2.1 合成数据（Synthetic data）
我们使用 $x$ 表示输入变量，使用 $t$ 表示目标变量，假定两个变量都在实轴上取连续值。假设我们有一个训练集，由 $N$ 个观测值 $x$ 组成，写作 $x_1,...,x_N$，同时有对应的观测值 $t$ 以 $t_1,...,t_N$ 表示。我们的目标是根据新的 $x$ 值预测 $t$。在没有见过的输入上给出准确预测的能力在机器学习中被称为泛化。

我们可以使用一个通过正弦曲线上采样生成的合成数据集来说明这个概念。图 1.4 展示了一个包含 $N=10$ 个数据点的图像，其中输入数据是从 $x_n,n=1,...,N$ 选择，这是 $N$ 个在 0-1 范围内均匀分布的点。相关的目标数据通过计算 $x$ 的 $sin(2x0$) 的值然后添加一些小的随机噪音（生成自高斯分布）来得到。通过这种生成数据的方式，我们可以理解很多现实数据集的重要特性，即它们都具有我们希望学习的潜在规律，但是每个观测都被随即噪音所影响。这种噪音可能本质上是随机的，例如放射性衰变。但是更多的情况下，噪声来自于一些未被观测到的源头。

![Figure1.4](images/fig1.4.png)
>Figure1.4：N = 10 个点的训练数据集图，显示为蓝色圆圈，每个点包含对输入变量 x 以及相应目标变量 t 的观察。绿色曲线显示用于生成数据的函数 sin(2 x)。我们的目标是在不了解绿色曲线的情况下，预测 x 的某个新值的 t 值

在这个示例中，我们知道生成数据的实际过程，即 $sin$ 函数，但是在现实情况下，我们的目标是从有限的训练数据中发现潜在的趋势。然而了解数据的生成过程有助于我们阐释机器学习的重要概念。

### 1.2.2 线性模型（Linear models）

我们的目标是利用这个训练集来预测新变量 $\^x$ 对应的 $\^t$。之后我们会发现这会涉及到发现隐藏的底层函数 $sin(2\pi x)$。从一个有限的数据集中泛化除整个函数本质上是一个非常困难的问题。而且观测数据还带有噪音，因此对于给定 $\^x$ 的合适的 $\^y$ 存在不确定性。_概率论_ 以精确且定量的方式提供了一种框架来表达这种不确定性，因此 _决策理论_ 允许我们利用这种概率表示来根据恰当的准测进行预测。从数据中学习概率就是机器学习的核心，并且本书会深刻探讨这个核心。

首先，我们考虑一种基于曲线拟合的简单方法。例如我们会用多项式来拟合
$$y(x,W)=w_0+w_1x+w_2x^2+...+w_Mx_M=\sum_{j=0}^{M}w_jx^i \tag{1.1}$$
$M$ 表示多项式的次数，$x_j$ 表示 $x$ 的 $j$ 次方。多项式的系数 $w_0,...,w_M$ 使用向量 $W$ 表示。即使多项式函数 $y(x,W)$ 是一个关于 $x$ 非线性函数，但它是一个关于系数 $W$ 的线性函数。这种关于未知参数的线性函数具有重要的特性和显著的局限性，它们被称为线性模型。

### 1.2.3 误差函数（Error function）

通过在训练集上拟合多项式，系数的值就可以确定。这可以通过最小化 _误差函数_ 来实现，误差函数是用于衡量给定 $W$ 的 $y(x,W)$ 在训练集上的不拟合程度的。一种广泛使用的简单的误差函数就是误差平方和，即对于给定的所有 $x_n$，预测值 $y(x_n,W)$ 和目标值 $t_n$ 之间的差异的平方的和。
$$
E(W)=\frac{1}{2}\sum_{n=1}^N\{y(w_n,W)-t_n\}^2 \tag{1.2}
$$
式中的 $\frac{1}{2}$ 是方便后续运算而设置。我们稍后会从概率论出发推导出这个误差函数。这里我们简单的说明，这是一个非负函数，当且仅当 $y(x,W)$ 精确的通过了训练数据中的每个点时才会为零。误差平方和函数的几何解释在图 1.5 中。

![Figure1.5](images/fig1.5.png)
> Figure1.5：误差函数 (1.2) 对应于函数 y(x; w) 中每个数据点的位移（由垂直绿色箭头所示）平方和（二分之一）

我们可以通过选择使得 $E(W)$ 的值尽可能小的 $W$ 来解决拟合问题。因为误差函数是 $W$ 的二次函数，其关于 $W$ 的导数将是线性函数，因此误差函数的最小值就会有一个唯一解，使用 $W^*$ 表示。最终我们将会得到 $y(x,W^*)$。

### 1.2.4 模型复杂度

还有一个问题是多项式的阶数 $M$，我们将看到这会是一个叫做 _模型复杂度_ 或 _模型选择_ 的概念的重要示例。在图 1.6 中，我们展示了采用阶数 $M=0,1,3,9$ 的多项式的拟合结果。

![Figure1.6](images/fig1.6.png)
> Figure 1.6

可以注意到，常量表达式$(M=0)$和一阶多项式$(M=1)$对数据的拟合效果很差，很难表达出函数 $sin(2\pi x)$。三阶多项式$(M=3)$似乎给出了最好的拟合效果。当我们使用更高阶的多项式$(M=9)$时，得到了对训练数据最好的拟合效果。实际上这个多项式几乎准确的通过了每个数据点并且 $E(W^*)=0$。然而拟合出来的曲线剧烈震荡并且对 $sin(2\pi x)$ 的拟合效果非常差。这种情况被称为 _过拟合_。

我们的目标实现好的泛化能力从而对新的数据也可以给出精确的预测。通过使用一个使用和训练集相同的生成程序生成的包含 100 个数据点的独立的 _测试集_ 我们可以定量的了解泛化性能对 $M$ 的依赖性。对于每个 $M$ 我们可以使用式 1.2 来评估训练集的 $E(W^*)$ 残差，也可以评估测试集的 $E(W^*)$。有时会使用更方便的均方根误差（RMS error）而不是误差函数 $E(W)$，定义如下：
$$
E_{RMS}=\sqrt{\frac{1}{N}\sum^N_{n=1}\{y(x_n,W)-t_n\}^2} \tag{1.3}
$$
除以 $N$ 是为了可以平等地对比不同大小的训练集，并且平方根保证了 $E_{RMS}$ 具有与目标变量 $t$ 相同的尺度。对于不同的 $M$，图 1.7 展示了训练集和测试集的均方根误差。

![Figure1.7](images/fig1.7.png)

测试集误差衡量了我们对于新的观测数据 $x$ 的预测效果有多好。从图中可以看出 $M$ 较小时会有比较大的测试误差，这可以被认为是多项式不够灵活难以捕捉到 $sin(w\pi x)$ 的震荡。当 $M$ 在 $3\leq M\leq 8$ 的范围内时，在测试集上有较小的测试误差，这些多项式也给出了 $sin(2\pi x)$ 的合理表示，在图 1.6 的 $M=3$ 图像就可以看出来。

对于 M = 9，训练集误差为零，正如我们所期望的，因为该多项式包含与 10 个系数 $w_0,...,w_9$ 相对应的 10 个自由度，可以精确调整到训练集中的 10 个数据点。然而，测试集误差已经变得非常大，并且如图 1.6 所示，相应的函数 $y(x; W^*)$ 表现出剧烈的振荡。

这可能看起来很矛盾，因为给定阶数的多项式包含所有低阶多项式作为特殊情况。因此，$M = 9$ 多项式能够生成至少与 $M = 3$ 多项式一样好的结果。此外，我们可能假设新数据的最佳预测器是生成数据的函数 $sin(2 x)$（稍后我们将看到情况确实如此）。我们知道函数 $sin(2 x)$ 的幂级数展开式包含所有阶项，因此我们可能期望随着 $M$ 的增加，结果应该持续改善。

我们可以通过检查系数 $w$ 的值来深入了解该问题？由各种阶次多项式求得，如表 1.1 所示。我们看到，随着 $M$ 的增加，系数的大小通常会变大。特别是对于 $M = 9$ 多项式，系数已根据数据进行了微调。它们具有较大的正值和负值，因此相应的多项式函数与每个数据点完全匹配，但在数据点之间（特别是接近范围的末端），该函数表现出图 1.6 中观察到的较大振荡。直观地说，所发生的情况是，具有较大 $M$ 值的更灵活的多项式越来越多地适应目标值上的随机噪声。

![Table1.1](images/table1.1.png)
> 表 1.1

通过检查学习模型在数据集大小变化时的行为，可以进一步了解这种现象，如图 1.8 所示。我们看到，对于给定的模型复杂度，随着数据集大小的增长，过度拟合问题变得不那么严重。另一种说法是，有了更大的数据集，我们就可以为数据拟合更复杂（换句话说更灵活）的模型。经典统计学中有时提倡的一种粗略观念是，数据点的数量应不少于模型中可学习参数数量的某个倍数（例如 5 或 10）。然而，当我们在本书后面讨论深度学习时，我们将看到使用参数明显多于训练数据点数量的模型也可以获得出色的结果。

![Figure1.8](images/fig1.8.png)
> 图 1.8

### 1.2.5 正则化（Regularization）

必须根据可用训练集的大小来限制模型中的参数数量，这是相当令人不满意的。根据所解决问题的复杂程度来选择模型的复杂度似乎更为合理。一种叫做 _正则化_ 的技术，是限制参数数量的替代方法，经常用于控制过度拟合现象，其原理是向误差函数 (1.2) 添加惩罚项，以阻止系数具有较大的值。最简单的惩罚项是所有系数的平方和，修正后的误差函数如下：
$$
\~E(W)=\frac{1}{2}\sum_{n=1}^N\{y(x_n,W)-t_n\}^2+\frac{\lambda}{2}||W||^2 \tag{1.4}
$$

其中 $‖w‖2 ≡ w^Tw = w^2_0 + w^2_1 + ...+ w^2_M$ ，并且由 $\lambda$ 系数控制正则化项与误差平方和项相比的权重。请注意，正则化项中通常会省略系数 $w_0$，因为包含它会导致结果取决于目标变量原点的选择（Hastie、Tibshirani 和 Friedman，2009），或者可能包含它但它具有自己的正则化系数。(1.4) 中的损失函数可以在封闭形式下有效的最小化。诸如此类的技术在统计文献中被称为收缩方法，因为它们会降低系数的值。在神经网络的背景下，这种方法称为权重衰减，因为神经网络中的参数称为权重，并且该正则化器鼓励它们向零衰减。

图 1.9 显示了将 $M = 9$ 阶多项式拟合到与之前相同的数据集，但现在使用 (1.4) 给出的正则化误差函数的结果。我们看到，对于 $ln\lambda = −18$ 的值，过度拟合已被抑制，我们现在获得了基础函数 $sin(2 x)$ 的更接近的表示。然而，如果我们使用太大的值，那么我们会再次获得较差的拟合效果，如图 1.9 中 $ln\lambda = 0$ 所示。表 1.2 中给出了拟合多项式的相应系数，表明正则化具有减小系数的大小的效果。

![Figure1.9](images/fig1.9.png)
> 图 1.9

![Table1.2](images/table1.2.png)
> 表 1.2

通过绘制训练集和测试集的 RMS 误差值 (1.3) 与 $ln\lambda$ 的关系，可以看出正则化项对泛化误差的影响，如图 1.10 所示。我们看到现在控制了模型的有效复杂度，从而决定了过度拟合的程度。

![Figure1.10](images/fig1.10.png)

### 1.2.6 模型选择（Model Selection）

